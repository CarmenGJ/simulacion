#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Números aleatorios no uniformes~
#+STARTUP: showall
:REVEAL_PROPERTIES:
# Template uses org export with export option <R B>
# Alternatives: use with citeproc
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Simulación">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session transformacion :exports both :results output org :tangle ../rscripts/02-aleatorios-nouniformes.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Números no uniformes.\\
*Objetivo*: En esta sección del curso veremos métodos para generar números aleatorios de distribuciones un poco mas generales que una distribución uniforme. En particular veremos algunos ejemplos de distribuciones continuas, discretas y mezclas. Además veremos una prueba de uniformidad adicional a la prueba Kolmogorov-Smirnov.\\
*Lectura recomendada*: Capítulo 2 de citep:Robert2010a. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

* Table of Contents                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#generación-de-variables-no-uniformes][Generación de variables no uniformes]]
  - [[#método-de-la-transformada-inversa][Método de la transformada inversa]]
    - [[#ejercicio][Ejercicio:]]
    - [[#tarea][Tarea:]]
  - [[#generalización][Generalización]]
    - [[#definición-función-inversa-generalizada][Definición [Función inversa generalizada]:]]
    - [[#teorema-de-la-función-inversa-generalizada][Teorema [de la función inversa generalizada]:]]
- [[#distribuciones-continuas][Distribuciones continuas]]
  - [[#variables-chi2][Variables $\chi^2$]]
  - [[#variables-gamma][Variables gamma]]
  - [[#variables-beta][Variables beta]]
    - [[#tarea][Tarea:]]
    - [[#código][Código:]]
  - [[#variables-gaussianas-correlacionadas][Variables Gaussianas correlacionadas]]
- [[#distribuciones-discretas][Distribuciones discretas]]
  - [[#binomial][Binomial]]
  - [[#poisson][Poisson]]
    - [[#propiedad-regla-empírica-o-regla-de-citetpukelsheim1994][Propiedad [Regla Empírica o regla de citet:Pukelsheim1994]:]]
- [[#mezclas][Mezclas]]
  - [[#binomial-negativa][Binomial negativa]]
- [[#prueba-chi2][Prueba $\chi^2$]]
  - [[#procedimiento-de-la-prueba-chi2][Procedimiento de la prueba $\chi^2$]]
    - [[#pregunta][Pregunta:]]
  - [[#aplicación-de-la-prueba][Aplicación de la prueba]]
  - [[#aplicación-de-pruebas][Aplicación de pruebas]]
- [[#referencias][Referencias]]
:END:

* Generación de variables no uniformes                           

~R~, por ejemplo, tiene distintos generadores de variables aleatorias. Un catálogo nos muestra que podemos
generar mucho mas variables que solamente una variable uniforme.

#+DOWNLOADED: screenshot @ 2022-08-15 19:03:50
#+attr_html: :width 1200 :align center
#+attr_latex: :width .95 \linewidth
#+caption: Catálogo de distribuciones en ~R~. 
[[file:images/20220815-190350_screenshot.png]]

#+REVEAL: split
En esta sección exploraremos los mecanismos tradicionales para generar números
pseudo-aleatorios de distribuciones un poco mas generales que una distribución
uniforme.

#+REVEAL: split

** Método de la transformada inversa

De su curso de cálculo de probabilidades se acordarán que si tenemos una
variable aleatoria $X$ con función de acumulación $\mathbb{P}$ y utilizamos $U =
\mathbb{P}(X)$, entonces $U$ es una variable aleatoria con distribución
$\mathsf{U}(0,1)$.

*** Ejercicio:
:PROPERTIES:
:reveal_background: #00468b
:END:
Considera $X \sim \mathsf{Exp}(1)$, de tal forma que $\mathbb{P}(x) = 1 -
e^{-x}$. Si sólo sabemos generar números uniformes, ¿cómo generarías números de
$X$?

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/exp-comparison.jpeg :exports results :results output graphics file
  g1 <- tibble(x = rexp(1000, 1)) |>
    ggplot(aes(x)) +
    geom_histogram() + sin_lineas +
    xlim(0, 8) + ggtitle("Exponencial R")

  g2 <- tibble(u = runif(1000),
               x = -log(u)) |>
    ggplot(aes(x)) +
    geom_histogram() + sin_lineas +
    xlim(0, 8) + ggtitle("Exponencial = f(Uniforme)")

  g1 + g2
#+end_src

#+RESULTS:
[[file:../images/exp-comparison.jpeg]]

*** Tarea:
:PROPERTIES:
:reveal_background: #00468b
:END:
Considera las siguientes distribuciones:
1. Logística con $\mu, \beta$ y función de acumulación
   \begin{align}
   \mathbb{P}(x) = \frac{1}{1 + e^{-(x - \mu)/\beta}}\,.
   \end{align}
2. Cauchy con parámetros $\mu, \beta$ y función de acumulación
   \begin{align}
   \mathbb{P}(x) = \frac{1}{2} + \frac{1}{\pi} \mathsf{arctan}((x - \mu)/\beta)\,.
   \end{align}

Genera números aleatorios con valores $\mu = 1$, $\beta = 2$ y comenta el rol de
cada parámetro en cada distribución.

** Generalización

En el caso anterior asumimos que la función inversa de la función de acumulación
existe. Sin embargo, no siempre es el caso. Necesitamos definir lo siguiente.

*** ~Definición~ [Función inversa generalizada]:
Sea $X$ una variable aleatoria con función de acumulación $\mathbb{P}$. La función inversa generalizada está definida como
\begin{align}
\mathbb{P}^{-1}(u) = \inf \{ x | F(x) \geq u\}\,. 
\end{align}


*** ~Teorema~ [de la función inversa generalizada]:
Sea $\mathbb{P}$ una función de distribución. Sea $\mathbb{P}^{-1}(\cdot)$  la función inversa generalizada de $\mathbb{P}$ y $U \sim \mathsf{U}(0,1)$ . Entonces, la variable aleatoria
$X = \mathbb{P}^{-1}(U)$ tiene distribución  $\mathbb{P}$. 

* Distribuciones continuas

Hay situaciones donde generar números aleatorios puede ser extremadamente
sencillo pues existe una relación entre la distribución que queremos generar y
ciertos componentes fáciles de simular.

** Variables $\chi^2$

Recordarán de su curso de cálculo de probabilidades que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = 2 \sum_{j = 1}^{\nu}X_j \sim \chi_{2 \nu}^2\,, 
\end{align}
con $\nu \in \mathbb{N}$.

** Variables gamma

También recordarán que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = \beta \sum_{j = 1}^{\alpha} X_j \sim \mathsf{Gamma}(\alpha, \beta)\,,
\end{align}
con $\alpha \in \mathbb{N}$.

** Variables beta

También recordarán que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = \frac{\sum_{j = 1}^{\alpha} X_j}{\sum_{j = 1}^{\alpha + \beta}X_j} \sim \mathsf{Beta}(\alpha, \beta)\,,
\end{align}
con $\alpha, \beta \in \mathbb{N}$.

*** Tarea:
:PROPERTIES:
:reveal_background: #00468b
:END:
Prueba las igualdades anteriores.

*** Código:

Por ejemplo, podemos utilizar el siguiente código para generar variables de una $\chi^2_6$ . 

#+begin_src R :exports both :results org
  set.seed(108)
  U <- runif(3 * 10^4)      # Genera uniformes
  U <- matrix(U, nrow = 3)  # Transforma a matriz
  X <- -log(U)              # Transforma a exponenciales
  X <- 2 * apply(X, 2, sum) # Suma los tres renglones
  summary(X) 
#+end_src

#+RESULTS:
#+begin_src org
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.151   3.505   5.420   6.064   7.926  27.904
#+end_src

#+REVEAL: split

A partir de la versión 4.1.1 ~R~ cuenta con un operador especial (~|>~) llamado ~pipe~ el
cual permite /anidar/ ciertas funciones y evitar la asignación repetitiva de
variables. 

#+begin_src R :exports both :results org
  set.seed(108)
  runif(3 * 10^4) |>        # Genera uniformes 
    matrix(nrow = 3) |>     # Transforma a matriz
    log() |>                # Calcula logaritmos
    apply(2, function(x){-2 * sum(x)} ) |> 
    summary()

#+end_src

#+RESULTS:
#+begin_src org
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.151   3.505   5.420   6.064   7.926  27.904
#+end_src

** Variables Gaussianas correlacionadas

Supongamos que queremos generar un par de variables $X \in \mathbb{R}^2$ de tal forma que
\begin{align}
X \sim \mathsf{N}\left( 0, \Sigma \right)\,,
\end{align}
donde $\Sigma_{ii} = 1$ para $i \in \{1,2\}$ y $\Sigma_{ij} = \rho$ con $i\neq
j$. Supongamos que sólo sabemos generar números aleatorios $\mathsf{N}(0,1)$.

#+REVEAL: split
¿Cómo generaríamos los vectores aleatorios que necesitamos?

#+REVEAL: split
¿Qué saben de propiedades matriciales de vectores aleatorios?

#+REVEAL: split
#+begin_src R :exports both :results org 
  set.seed(108)
  Sigma <- diag(2); Sigma[1,2] <- .75; Sigma[2,1] <- .75;
  L <- chol(Sigma)

  Z <- rnorm(2 * 10^4)      # Generamos vectores estandar
  Z <- matrix(Z, nrow = 2)  # Reacomodamos en matriz
  X <- t(L) %*% Z           # Transformacion lineal
  cov(t(X))
#+end_src

#+RESULTS:
#+begin_src org
       [,1]   [,2]
[1,] 1.0173 0.7772
[2,] 0.7772 1.0312
#+end_src

#+BEGIN_NOTES
El operador ~%*%~ ejemplifica uno de las limitantes por diseño de ~R~. Pues no está
hecho para realizar operaciones vectoriales de manera nativa. Por ejemplo, en
~Matlab~ las operaciones son nativas y en ~python~ a través de ~numpy~ las operaciones
matriciales también (y parte de los métodos).
#+END_NOTES

* Distribuciones discretas

Ahora, veremos algunas técnicas generales para distribuciones discretas. O mejor
dicho, para generar números aleatorios con soporte en los enteros.

#+REVEAL: split
Supongamos que nuestro objetivo es poder generar de una $X\sim
\mathbb{P}_\theta$ donde $X\in \mathbb{Z}$. La estrategia es ~guardar todas las
probabilidades del soporte~. Es decir, calcular
\begin{align}
p_0 = \mathbb{P}_\theta(X \leq 0)\,, \quad p_1 = \mathbb{P}_\theta(X \leq 1)\,, \,\ldots\,,
\end{align}
generar $U \sim \mathsf{U}(0,1)$ y establecer
\begin{align}
X = k \text{ si } \, p_{k-1} < U < p_k\,.
\end{align}

** Binomial
Supongamos que nos interesa $X \sim \mathsf{Bin}(10, 0.3)$, el vector de probabilidades lo podemos calcular con la función ~pbinom(k, 10, .3)~.

#+begin_src R :exports both :results org 
  k <- 1:10
  probs <- pbinom(k, 10, .3)
  probs
#+end_src

#+RESULTS:
#+begin_src org
 [1] 0.1493 0.3828 0.6496 0.8497 0.9527 0.9894 0.9984 0.9999 1.0000 1.0000
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  rbinomial <- function(nsamples, size, theta){
    probs <- pbinom(k, size, theta)
    x <- c()
    for (jj in 1:nsamples){
      u <- runif(1)
      x[jj] <- which(probs > u)[1]
    }
    return(x)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudobinomial-histogram.jpeg :exports results :results output graphics file
  set.seed(108)
  x <- rbinomial(1000, 10, .3)
  tibble(samples = x) |>
    ggplot(aes(samples)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1,
                   color = 'white') +
    geom_line(data = tibble(x_ = 1:8, y_ = dbinom(x_, 10, .3)),
            aes(x_, y_), lwd = 1.5, lty = 2, 
            colour = "salmon") + 
    sin_lineas
#+end_src

#+RESULTS:
[[file:../images/pseudobinomial-histogram.jpeg]]

** Poisson

Ahora supongamos que nos interesa simular de una Poisson con parámetro $\lambda = 7$.

#+REVEAL: split
¿Cuál es el soporte de una $\mathsf{Bin}(10, .3)$? ¿Cuál es el soporte de una $\mathsf{Poisson}(7)$?

#+REVEAL: split
Tenemos que guardar las probabilidades
#+begin_src R :exports both :results org
  k <- 1:24
  ppois(k, 7)
#+end_src

#+RESULTS:
#+begin_src org
 [1] 0.007295 0.029636 0.081765 0.172992 0.300708 0.449711 0.598714 0.729091
 [9] 0.830496 0.901479 0.946650 0.973000 0.987189 0.994283 0.997593 0.999042
[17] 0.999638 0.999870 0.999956 0.999986 0.999995 0.999999 1.000000 1.000000
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  rpoisson <- function(nsamples, lambda){
    probs <- ppois(1:30, lambda)
    x <- c()
    for (jj in 1:nsamples){
      u <- runif(1)
      x[jj] <- which(probs > u)[1]
    }
    return(x)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudopoisson-histogram.jpeg :exports results :results output graphics file
  set.seed(108)
  x <- rpoisson(1000, 7)
  tibble(samples = x) |>
    ggplot(aes(samples)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1,
                   color = 'white') +
    geom_line(data = tibble(x_ = 1:30, y_ = dpois(x_, 7)),
            aes(x_, y_), lwd = 1.5, lty = 2, 
            colour = "salmon") + 
    sin_lineas
#+end_src

#+RESULTS:
[[file:../images/pseudopoisson-histogram.jpeg]]

#+REVEAL: split
El problema de generar números aleatorios de la manera anterior es la necesidad de /guardar/ el vector de probabilidades. Por ejemplo, una $\mathsf{Poisson}(100)$. El intervalo $\lambda \pm 3 \sqrt{\lambda}$ es $(70, 130)$.

*** ~Propiedad~ [Regla Empírica o regla de citet:Pukelsheim1994]:
Si $X$ es una variable aleatoria con media y varianza finitas. Entonces, la probabilidad de que una realización de $X$ se encuentre a mas de 3 desviaciones estándar de la media es a lo mas $5\%$. 

* Mezclas 

Otra familia de distribuciones que es muy interesante de simular son las
mezclas. Es decir, cuando podemos escribir
\begin{align}
f(x) = \int_\mathcal{Y} g(x | y) \, p(y) \, \text{d}y\,, \quad \text{ o } \quad f(x) = \sum_{i \in \mathcal{Y}}^{} p_i \, f_i(x)\,,
\end{align}
siempre y cuando sea sencillo generar números aleatorios de las marginales.

#+REVEAL: split
Por ejemplo, para generar números aleatorios de una $t$ Student con $\nu$ grados
de libertad. Podemos usar la representación
\begin{align}
X | y \sim \mathsf{N}(0, \nu/y)\,, \quad Y \sim \chi^2_\nu\,.
\end{align}

** Binomial negativa

La variable aleatoria $X\sim \mathsf{BinNeg}(n, \theta)$ tiene una representación
\begin{align}
X | y \sim \mathsf{Poisson}(y)\,, \quad Y \sim \mathsf{Gamma}(n , \beta )\,,
\end{align}
donde $\beta = (1-\theta)/\theta$.

#+begin_src R :exports code :results none
  nsamples <- 10^4
  n <- 6; theta <- .3
  y <- rgamma(nsamples, n, rate = theta/(1-theta))
  x <- rpois(nsamples, y)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rbinneg-histogram.jpeg :exports results :results output graphics file
  tibble(samples = x) |>
  ggplot(aes(x)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1, color = "white") +
    geom_line(data = tibble(x_ = 1:60, y_ = dnbinom(x_, n, theta)),
              aes(x_, y_), lwd = 1.5, lty = 2, 
              colour = "salmon") +
    sin_lineas
#+end_src

#+RESULTS:
[[file:../images/rbinneg-histogram.jpeg]]


* Prueba $\chi^2$

Podemos usar otro mecanismo para probar estadísticamente si nuestros números
pseudo aleatorios siguen la distribución que deseamos.

Podemos pensar en esta alternativa como la versión ~discreta~ de la prueba ~KS~.

Lo que estamos poniendo a prueba es
\begin{align}
H_0: \mathbb{P}(x) = \mathbb{P}_0(x) \,\, \forall x\, \quad \text{ contra } \quad H_1: \mathbb{P}(x) \neq \mathbb{P}_0(x) \text{ para alguna } x\,.
\end{align}

** Procedimiento de la prueba $\chi^2$

1. Hacemos una partición del rango de la distribución supuesta en $k$
   subintervalos con límites $\{a_0, a_1, \ldots, a_k\}$, y definimos $N_j$ como
   el número de observaciones (de nuestro generador de pseudo-aleatorios) en
   cada subintervalo.

2. Calculamos la proporción esperada de observaciones en el intervalo $(a_{j-1},
   a_j]$ como
   \begin{align}
   p_j = \int_{a_{j-1}}^{a_j} \text{d} \mathbb{P}(x)\,.
   \end{align}

3. La estadística de prueba es
   \begin{align}
   \chi^2 = \sum_{j = 1}^{k} \frac{(N_j - p_j)^2}{n p_j}\,.
   \end{align}  

#+BEGIN_NOTES
Nota que estamos comparando dos histogramas. El histograma observado que
construimos a partir de nuestros números pseudo-aleatorios contra el histograma
que esperaríamos de la distribución. ¿Puedes pensar en algún problema con esta
prueba?
#+END_NOTES

#+REVEAL: split
La visualización correspondiente sería lo siguiente. Utilizamos nuestro generador para obtener muestras. 

#+begin_src R :exports code :results none 
  ## Esto es para poner a prueba un pseudo generador 
  rpseudo.uniform <- function(nsamples, seed = 108727){
    x0 <- seed; a <- 7**5; m <- (2**31)-1;
    x  <- x0; 
    for (jj in 2:nsamples){
      x[jj] <- (a * x[jj-1]) %% m
    }
    x/m
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudo-uniform-hist.jpeg :exports results :results output graphics file
  nsamples <- 30;  nbins <- 10;
  samples <- data.frame(x = rpseudo.uniform(nsamples, seed = 166136))
  samples |>
  ggplot(aes(x)) +
    geom_hline(yintercept = nsamples/nbins, color = "darkgray", lty = 2) +
    annotate("rect",
             ymin = qbinom(.95, nsamples, 1/nbins),
             ymax = qbinom(.05, nsamples, 1/nbins),
             xmin = -Inf, xmax = Inf,
             alpha = .4, fill = "gray") + 
    geom_histogram(bins = nbins, color = "white") + sin_lineas +
    ggtitle("Semilla: 166136")
#+end_src

#+RESULTS:
[[file:../images/pseudo-uniform-hist.jpeg]]


*** Pregunta:
:PROPERTIES:
:reveal_background: #00468b
:END:
¿Qué esperaríamos de nuestro estadístico $\chi^2$ si nuestro generador de pseudo-aleatorios es incorrecto?

** Aplicación de la prueba

#+begin_src R :exports none :results none 
  ## Esto es para poner a prueba un pseudo generador =============================
  rpseudo.uniform <- function(nsamples, seed = 108727){
    x0 <- seed; a <- 7**5; m <- (2**31)-1;
    x  <- x0; 
    for (jj in 2:nsamples){
      x[jj] <- (a * x[jj-1]) %% m
    }
    x/m
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  nsamples <- 30; nbreaks <- 10
  samples <- data.frame(x = rpseudo.uniform(nsamples))

  Fn <- hist(samples$x, breaks = nbreaks, plot = FALSE)$counts/nsamples
  F0 <- 1/nbreaks

  X2.obs <- (nsamples*nbreaks)*sum((Fn - F0)**2)  
#+end_src


#+REVEAL: split
#+begin_src R :exports code :results none
  ## Esto es para generar datos observados de la distribucion que queremos 
  experiment <- function(nsamples){
    nbreaks <- 10
    samples <- data.frame(x = runif(nsamples))
    Fn <- hist(samples$x, breaks = nbreaks, plot = FALSE)$counts/nsamples
    F0 <- 1/nbreaks
    X2 <- (nsamples*nbreaks)*sum((Fn - F0)**2)
    return(X2)
  }

  X2 <- c()
  for (jj in 1:5000){
    X2[jj] <- experiment(nsamples)
  }
#+end_src

#+REVEAL: split
En la [[fig-chisq-hist]] se muestra el histograma de las réplicas del estadístico
$\chi^2$ bajo el generador uniforme (lo tomamos como la distribución de la
hipótesis nula) y comparamos contra el observado (línea punteada). Adicional, se
incorpora la densidad de una $\chi^2_{k-1}$ (leáse ji-cuadrada con $k-1$ grados
de libertad) que es la distribución asintótica del estadístico.

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/chi2-histograma.jpeg :exports results :results output graphics file
  data.frame(estadistica = X2) |>
    ggplot(aes(estadistica)) +
    geom_histogram(aes(y = ..density..)) +
    geom_vline(xintercept = X2.obs, lty = 2, color = 'red', lwd = 1.5) +
    stat_function(fun = dchisq, args = list(df = nbreaks - 1), color = 'salmon', lwd = 1.5) +
    sin_lineas + xlab(expression(chi^{2}))
#+end_src
#+name: fig-chisq-hist
#+RESULTS:
[[file:../images/chi2-histograma.jpeg]]

#+REVEAL: split
Por lo tanto, la probabilidad de haber observador una estadístico $\chi^2$ tan extremo como el que observamos si el generador hubiera sido el que suponemos es:
#+begin_src R :exports results :results org 
  print(paste("Estadistico: ", round(X2.obs, 4), ", Probabilidad: ", mean(X2 >= X2.obs), sep =''))
#+end_src

#+RESULTS:
#+begin_src org
[1] "Estadistico: 12.6667, Probabilidad: 0.177"
#+end_src

Que podemos comparar contra el que obtenemos de una prueba "tradicional":
#+begin_src R :exports both :results org 
  counts.obs <- Fn*nsamples 
  chisq.test(counts.obs, p = rep(1, nbreaks)/nbreaks, simulate.p.value = TRUE)
#+end_src

#+RESULTS:
#+begin_src org

	Chi-squared test for given probabilities with simulated p-value (based
	on 2000 replicates)

data:  counts.obs
X-squared = 13, df = NA, p-value = 0.2
#+end_src

#+REVEAL: split
- La prueba $\chi^2$  pues usualmente no es buena cuando el número de observaciones es menor a 50.
- La prueba ~KS~ tiene mejor potencia que la prueba $\chi^2$:
  #+begin_src R :exports both :results org 
     ks.test(samples$x, "punif")
  #+end_src

  #+RESULTS:
  #+begin_src org

          Exact one-sample Kolmogorov-Smirnov test

  data:  samples$x
  D = 0.16, p-value = 0.4
  alternative hypothesis: two-sided
  #+end_src

** Aplicación de pruebas

En la práctica se utiliza una colección de pruebas pues cada una es sensible a
cierto tipo de desviaciones. La bateria de pruebas mas utilizada es la colección
de pruebas ~DieHARD~ que desarrolló [[https://en.wikipedia.org/wiki/Diehard_tests][George Marsaglia]] y que se ha ido
complementando con los años.

* Referencias

bibliographystyle:abbrvnat
bibliography:references.bib
