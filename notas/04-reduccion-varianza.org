#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Técnicas de reducción de varianza~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/04-reduccion-varianza.pdf
:END:
#+PROPERTY: header-args:R :session varianza :exports both :results output org :tangle ../rscript/04-reduccion-varianza.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc noexport

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Reducción de varianza.\\
*Objetivo*: Que veremos.\\
*Lectura recomendada*: Referencia.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#error-monte-carlo][Error Monte Carlo]]
- [[#variables-antitéticas][Variables antitéticas]]
  - [[#fundamento][Fundamento]]
    - [[#preguntas][Preguntas:]]
    - [[#conclusiones][Conclusiones:]]
    - [[#consideraciones][Consideraciones:]]
  - [[#ejemplo-integral-en-intervalo][Ejemplo: Integral en intervalo]]
- [[#variables-de-control][Variables de Control]]
  - [[#consideraciones][Consideraciones:]]
  - [[#ejemplo][Ejemplo]]
    - [[#pregunta][Pregunta:]]
- [[#monte-carlo-condicional][Monte Carlo condicional]]
  - [[#ejemplo-mezcla-beta-binomial][Ejemplo: Mezcla Beta-Binomial]]
  - [[#ejemplo-mezcla-poisson-beta][Ejemplo: Mezcla Poisson-Beta]]
  - [[#ejemplo-estimación-de-densidades][Ejemplo: Estimación de densidades]]
:END:

* Introducción

Ya vimos algunos casos donde podemos reducir la varianza de nuestros estimadores
Monte Carlo. Esto nos ayuda a mejorar la velocidad y eficiencia estadística de
nuestros estimadores y en consecuencia optimizar los recursos computacionales.

Para lograrlo, por ejemplo, consideramos la posibilidad de cambiar la densidad
contra la que estamos realizando el proceso de integración. A esta distribución
le llamaremos ~medida de referencia~.

En esta sección estudiaremos técnicas que nos permitirán reducir la varianza de
nuestros estimadores.

** Error Monte Carlo

Lo que estamos tratando de resolver es el problema de
\begin{align}
\theta = \mathbb{E}_\pi(h(X))\,,
\end{align}
por medio de
1. Generar  muestras $X_{1}, \ldots, X_{N} \overset{\mathsf{iid}}{\sim} \pi$.
2. Estimar por medio de $\hat \theta_N = (1/N) \sum_{i = 1}^{N} h(X_i)$.

#+REVEAL: split
Bajo ciertas condiciones, un intervalo de confianza ($1-\delta$) puede construirse por medio
\begin{align}
[\hat \theta_N - z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N), \hat \theta_N + z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N)]\,,
\end{align}
donde podemos calcular el error estándar del estimador.

#+REVEAL: split
Hemos jugado con la noción de medir la calidad de nuestro estimador Monte
Carlo al observar la longitud del intervalo. Es por esto que utilizamos la
~longitud media~ (~HW~) del intervalo de confianza
\begin{align}
\mathsf{HW}= z_{1-\delta/2} \, \mathsf{ee}(\hat \theta_N)\,.
\end{align}
 
#+REVEAL: split
Veremos técnicas de reducción de varianza que nos ayudarán a reducir la longitud media.

#+BEGIN_NOTES
El uso de técnicas de reducción de varianza nos obligan a conocer un poco más
sobre el modelo que está detrás de nuestro estimador. Esto es, para mejorar
nuestra estimación $\hat \pi_N^{\mathsf{MC}}(f)$ tenemos que conocer más
propiedades tanto de $f$ y/o $\pi$. Pues esto nos ayudará a reducir aún mas
nuestra varianza.
#+END_NOTES


* Variables antitéticas

- Lo que buscaremos es inducir una correlación negativa entre secuencias de números pseudo-aleatorios.
- La idea es que al generar números en pares una observación grande en la primera secuencia se compense con una observación pequeña en la segunda.
- El ejemplo típico es sincronizar  $u_n \sim \mathsf{U}(0,1)$ con $u_n' = 1 - u_n$.
\newpage
** Fundamento

Supongamos que tenemos $(X^{(1)}_{1}, \ldots, X^{(1)}_{N})$ y $(X^{(2)}_{1}, \ldots, X^{(2)}_{N})$ en donde
para generar $X^{(1)}_j$ se utilizó $u_j$ y para generar $X^{(2)}_j$ se utilizó $1 - u_j$.

*** ~Preguntas~:
:PROPERTIES:
:reveal_background: #00468b
:END:
1. ¿Cuál es el valor esperado de $X^{(1)}_j$ y $X^{(2)}_j$?
2. ¿Son independientes?
3. ¿Qué pasa con los pares $(X^{(1)}_j, X^{(2)}_j)$ y $(X^{(1)}_k, X^{(2)}_k)$?



*** ~Conclusiones~:
Por lo anterior, si definimos
\begin{align}
X_j = \frac{X^{(1)}_j + X^{(2)}_j}{2}\,, \qquad \bar X_N = \frac1N \sum_{n = 1}^{N} X_n\,,
\end{align}
tenemos un estimador que tiene la siguientes propiedades:
1. Es insesgado.
2. Tiene menor varianza que una muestra de $2N$ simulaciones.


*** ~Consideraciones~:
No siempre se puede lograr el objetivo. Es decir, depende del modelo.

** Ejemplo: Integral en intervalo

Queremos estimar $\int_{a}^{b} f(x) \text{d}x$. El estimador Monte Carlo sería
\begin{align}
\hat \pi_N^{\mathsf{MC}}(f) = \frac{b-a}{N} \sum_{n = 1}^{N} f(x_n)\,,
\end{align}
donde $x_n \sim \mathsf{U}(a, b)$.

#+REVEAL: split
Si escogemos la mitad (aleatoriamente) y por cada muestra usamos $x'_n = a + (b - x_n)$.
Entonces tendríamos 
\begin{align}
\hat \pi_N^{\mathsf{AMC}}(f) = \frac{b-a}{N/2} \sum_{n = 1}^{N/2} \frac{f(x_n) + f(x'_n)}{2}\,,
\end{align}

#+begin_src R :exports code :results none
  set.seed(108)
  nsamples <- 10^3;
  a <- 2; b <- 3;
  u <- runif(nsamples, min = a, max = b)
  x <- dnorm(u)
#+end_src

#+begin_src R :exports results :results org 
  c(estimador = mean(x), error.std = sd(x)/sqrt(nsamples), N = length(x))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std         N 
2.153e-02 1.396e-02 1.000e+03
#+end_src

#+begin_src R :exports code :results none 
  u_ <- a + (b - u)
  x_ <- dnorm(u_)
  x  <- (x + x_)/2
  ax <- x[1:(nsamples/2)]
#+end_src

#+begin_src R :exports results :results org 
  c(estimador = mean(ax), error.std = sd(ax)/sqrt(nsamples), N = length(ax))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std         N 
2.133e-02 3.518e-03 5.000e+02
#+end_src


* Variables de Control

Supongamos que queremos estimar $\mathbb{E}(X)$ y tenemos acceso a una variable aleatoria $Y$ que está ~correlacionada~ y se conoce $\nu = \mathbb{E}(Y)$. A $Y$ se le conoce como ~variable control~ de $X$.

#+REVEAL: split
Sea $X_c = X - a ( Y - \nu)$. Entonces
1. $\mathbb{E}(X_c) = \mathbb{E}(X)$.
2. $\mathbb{V}(X_c) = \mathbb{V}(X - a ( Y - \nu)) = \mathbb{V}(X) + a^2 \mathbb{V}(Y) - 2 a \mathsf{Cov}(X,Y)$. Esto implica que
   \begin{align}
   \mathbb{V}(X_c) \leq \mathbb{V}(X)\, \quad \text{ si }  \quad 2 a \mathsf{Cov } (X,Y) > a^2 \mathbb{V}(Y)\,.
   \end{align}
3. El caso particular
   \begin{align}
   a^* = \frac{\mathsf{Cov}(X,Y)}{\mathbb{V}(Y)}\,,
   \end{align}
   que induce la mínima varianza.
4. En este último caso
   \begin{align}
   \mathbb{V}(X_c) = (1 - \rho^2_{X,Y}) \mathbb{V}(X)\,.
   \end{align}


** Consideraciones:
En la práctica no siempre se conoce el valor de $\mathbb{V}(Y)$ y muy difícilmente la $\mathsf{Cov}(X,Y)$, lo que implica que es difícil conocer el valor de $a$. 

#+REVEAL: split
En la práctica se puede utilizar un estudio piloto para estimar $a$ citep:Lavenberg1982. Esto es,
\begin{align}
\hat a_M = \frac{\widehat{\mathsf{Cov}}_M(X,Y)}{\widehat{\mathbb{V}}_M(Y)}\,.
\end{align}
Nota que el estimador resultante para la media de $X_c$ ya no es un estimador insesgado.

** Ejemplo

Supongamos que $X \sim \mathsf{N}(0,1)$ y que $f(X)= \frac{X^6}{1 + X^2}$.

- Entonces, utilizando la igualdad
  \begin{align}
  \frac{x^6}{1 + x^2} = x^4 - x^2 + 1 - \frac{1}{1 + x^2}\,,
  \end{align}
  y podemos aproximar con $Y = g(X)= x^4 - x^2 + 1$.
- Para esta elección tenemos $\mathbb{E}(Y) = 3$.
- Asi que el problema se reduce a
  \begin{align}
  \mathbb{E} \left[  \frac{X^6}{1 + X^2}\right] = 3 - \mathbb{E} \left[ \frac{1}{1 + X^2}\right]\,.
  \end{align}


#+REVEAL: split
#+begin_src R :exports code :results none 
  set.seed(108)
  x <- rnorm(nsamples)
#+end_src

#+begin_src R :exports both :results org 
  f_x <- x**6/(1 + x**2)
  c(estimador = mean(f_x), error.std = sd(f_x)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3473    0.2798
#+end_src

#+begin_src R :exports both :results org 
  g_x <- 3 - 1 / (1 + x**2)
  c(estimador = mean(g_x), error.std = sd(g_x)/sqrt(nsamples) )
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 2.343346  0.008549
#+end_src

*** ~Pregunta~:
:PROPERTIES:
:reveal_background: #00468b
:END:
¿Por qué estos estimadores dan los mismas números que con el código anterior? 

#+begin_src R :exports both :results org
  set.seed(108)
  x <- rnorm(100 * nsamples)
  x <- array(x, c(100, nsamples))
  f_x <- x**6/(1 + x**2)
  estimadores <- apply(f_x, 1, mean)
  c(estimador = mean(estimadores), error.std = sd(estimadores))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3473    0.2752
#+end_src

#+begin_src R :exports both :results org 
  g_x <- 3 - 1/(1+x**2)
  estimadores <- apply(g_x, 1, mean)
  c(estimador = mean(estimadores), error.std = sd(estimadores))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
   2.3453    0.0081
#+end_src


* Monte Carlo condicional

Se pueden utilizar algunos resultados teóricos intermedios para algunos casos. A esta técnica también se le conoce como método ~Rao-Blackwell~. 

#+REVEAL: split
Supongamos que nos interesa $\mathbb{E}(f(X))$ y del alguna manera tenemos conocimiento de una variable aleatoria que está relacionada con la original por medio de $\mathbb{E}(f(X) |Z = z)$. Utilizando la propiedad torre podemos calcular
\begin{align}
\mathbb{E}(f(X)) = \mathbb{E}\left( \mathbb{E}(f(X) | Z = z) \right) \,.
\end{align}

Donde además tenemos que
\begin{align}
\mathbb{V}(f(X)) = \mathbb{V}(E(f(X)|Z)) + \mathbb{E}(\mathbb{V}(f(X)|Z))\,.
\end{align}

#+REVEAL: split
Lo que buscamos es que:
1. $Z$ pueda ser generado de manera eficiente.
2. Se pueda calcular $\mathbb{E}(f(X)|Z)$.
3. El valor de $\mathbb{E}(\mathbb{V}(f(X)|Z))$ sea grande. 

#+REVEAL: split
Por lo tanto, el método es:
1. Generar una muestra $Z_{1}, \ldots, Z_{N} \overset{\mathsf{iid}}{\sim} \pi(Z)$ .
2. Calcular $\mathbb{E}(f(X)| Z = z_k)$ de manera analítica.
3. Calcular el estimador de $\pi(f) = \mathbb{E}(f(X))$ por medio de
   \begin{align}
   \hat \pi_N^{\mathsf{CMC}} (f) = \frac1N \sum_{n = 1}^{N} \mathbb{E}(f(X)| Z = Z_k)\,.
   \end{align}
   


** Ejemplo: Mezcla Beta-Binomial

Supongamos un modelo Beta-Binomial. Igual que antes asumamos $n = 20$ y $\alpha = 2, \beta = 5$.

#+begin_src R :exports both :results org 
  set.seed(108)
  theta <- rbeta(nsamples, 2, 5)
  y <- rbinom(nsamples, size = 20, theta)
  c(estimador = mean(y), error.std = sd(y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
    5.585     0.119
#+end_src

#+begin_src R :exports both :results org 
  m_y <- 20 * theta
  c(estimador = mean(m_y), error.std = sd(m_y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
    5.587     0.102
#+end_src

El porcentaje de reducción de varianza es
#+begin_src R :exports results :results org 
  (sd(y) - sd(m_y))/sd(y)
#+end_src


** Ejemplo: Mezcla Poisson-Beta

Supongamos un modelo de mezcla
#+begin_src R :exports both :results org 
  set.seed(108)
  w <- rpois(nsamples, 10)
  y <- rbeta(nsamples, w, w**2 + 1)
  c(estimador = mean(y), error.std = sd(y)/sqrt(nsamples))

#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 0.096535  0.001404
#+end_src

#+begin_src R :exports both :results org 
  m_y <- w / (w**2 + w + 1)
  c(estimador = mean(m_y), error.std = sd(m_y)/sqrt(nsamples))
#+end_src

#+RESULTS:
#+begin_src org
estimador error.std 
 0.098341  0.001019
#+end_src

El porcentaje de reducción de varianza es
#+begin_src R :exports results :results org 
  (sd(y) - sd(m_y))/sd(y)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.2737
#+end_src


** Ejemplo: Estimación de densidades

Podemos utilizar el método Monte Carlo condicionado para estimar densidades. Por ejemplo, si consideramos que $X_{1}, \ldots, X_{k} \overset{\mathsf{iid}}{\sim} \pi$ y nos interesa $S_k = X_{1} + \cdots + X_{k}$. Nos podemos preguntar por al densidad de la suma. Sabemos que la densidad es un objeto infinitesimal $\mathbb{P}(S_k \in \text{d}x)$. Y en algunas situaciones no tenemos acceso a éste.

Por ejemplo, consideremos $X_i \sim \mathsf{Pareto}(1, \alpha = 3/2)$. Para este caso, no se puede escribir la densidad de $S_k$ para $k > 1$. Lo que si sabemos es que
\begin{align}
S_k \, | \, S_{k-1} \overset{\mathsf{d}}{=} X_k \, |\, S_{k-1} \sim \mathsf{Pareto}(S_{k-1}, \alpha)\,.
\end{align}
Por lo que podemos estimar la densidad de $X_k \,|\, S_{k-1}$ para valores, por ejemplo, en $[0, 15)$.


#+begin_src R :exports code :results none
  nsamples <- 5 * 10^3; ngrid <- 1000
  rpareto <- function(n, alpha) {1 / runif(n)^(1/alpha) - 1}
  dpareto <- function(x, alpha) { ifelse( x >= 0, (alpha / ((x+1)**(alpha + 1))), 0) }
  k <- 4
  u <- rpareto( (k-1) * nsamples, alpha = 3/2)
  u <- array(u, c(k-1, nsamples))
  S <- apply(u, 2, sum)
  x <- seq(0.1, 15, length.out = ngrid)
#+end_src

#+begin_src R :exports code :results none 
  estimador <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, mean)

  error.std <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, sd)
#+end_src

#+begin_src R :exports none :results none
  k <- 8
  u <- rpareto( (k-1) * nsamples, alpha = 3/2)
  u <- array(u, c(k-1, nsamples))
  S <- apply(u, 2, sum)

  estimador.8 <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, mean)

  error.std.8 <- array(x, c(ngrid,1)) |>
    apply(1, FUN = function(x_){ dpareto(x_ - S, alpha = 3/2) }) |>
    apply(2, sd)
#+end_src

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pareto-density-estimate.jpeg :exports results :results output graphics file
  g1 <- tibble(x, estimador, error.std) |>
  ggplot(aes(x, estimador)) +
    geom_ribbon(aes(ymin = estimador - 2 * error.std/sqrt(nsamples),
                    ymax = estimador + 2 * error.std/sqrt(nsamples)),
                fill = 'salmon', alpha = .3) + 
    geom_line() + sin_lineas + ggtitle(expression(k==4))

  g2 <- tibble(x, estimador = estimador.8, error.std = error.std.8) |>
  ggplot(aes(x, estimador)) +
    geom_ribbon(aes(ymin = estimador - 2 * error.std/sqrt(nsamples),
                    ymax = estimador + 2 * error.std/sqrt(nsamples)),
                fill = 'salmon', alpha = .3) + 
    geom_line() + sin_lineas + ggtitle(expression(k==8))

  g1 + g2
#+end_src
#+caption: Densidad de $x \,|\, S_{k-1}$. 
#+RESULTS:
[[file:../images/pareto-density-estimate.jpeg]]

bibliographystyle:abbrvnat
bibliography:references.bib

* Muestreo por importancia :noexport:

** Muestreo estratificado

