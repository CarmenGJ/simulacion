#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Integración Monte Carlo~
#+STARTUP: showall
:REVEAL_PROPERTIES:
# Template uses org export with export option <R B>
# Alternatives: use with citeproc
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Simulación">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session intro :exports both :results output org :tangle ../rscripts/03-montecarlo.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc noexport latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022.\\
*Objetivo*. Estudiar integración numérica en un contexto probabilístico. Estudiar,
 en particular, el método Monte Carlo y entender sus bondades y limitaciones como un
 método de aproximación de integrales. \\
*Lectura recomendada*: Una lectura mas técnica sobre reglas de cuadratura se
puede encontrar en la sección 3.1 de citet:Reich2015. Y una buena referencia
(técnica) sobre el método Monte Carlo lo encuentran en citet:Sanz-Alonso2019.
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#integración-numérica][Integración numérica]]
  - [[#análisis-de-error][Análisis de error]]
  - [[#más-de-un-parámetro][Más de un parámetro]]
  - [[#reglas-de-cuadratura][Reglas de cuadratura]]
- [[#integración-monte-carlo][Integración Monte Carlo]]
  - [[#ejemplo-dardos][Ejemplo: Dardos]]
  - [[#propiedades][Propiedades]]
    - [[#teorema-error-monte-carlo][Teorema [Error Monte Carlo]​]]
    - [[#teorema-tlc-para-estimadores-monte-carlo][Teorema [TLC para estimadores Monte Carlo]​]]
    - [[#nota][Nota:]]
    - [[#nota][Nota:]]
    - [[#nota][Nota:]]
  - [[#estimación-de-una-proporción][Estimación de una proporción]]
    - [[#tarea][Tarea:]]
    - [[#pregunta][Pregunta:]]
- [[#desigualdades-de-concentración][Desigualdades de concentración]]
  - [[#desigualdad-de-chebyshev][Desigualdad de Chebyshev]]
    - [[#teorema-desigualdad-de-chebyshev][Teorema [Desigualdad de Chebyshev]:]]
    - [[#ejercicio][Ejercicio:]]
    - [[#solución][Solución:]]
  - [[#desigualdad-de-hoeffding][Desigualdad de Hoeffding]]
:END:


* Introducción

En muchas aplicaciones nos interesa poder resolver integrales de manera numérica. Estas pueden ser de cualquier forma. Por ejemplo, nos puede interesar resolver
\begin{align}
\int_{\Theta}^{} h(\theta) \, \text{d}\theta\,,
\end{align}
que bien puede ser reexpresada como una integral bajo una medida de
probabilidad.

#+REVEAL: split
Por ejemplo,
\begin{align}
\int_{\Theta}^{} f(\theta) \, \pi(\theta ) \,  \text{d}\theta\,,
\end{align}
de tal forma que podemos pensar en la ecuación de arriba como un valor esperado
de una variable $\theta \sim \pi(\cdot)$ y calcular mediante un método numérico.

#+REVEAL: split
#+ATTR_REVEAL: :frag (appear)
- La pregunta clave (I) es: ¿qué distribución podemos utilizar?
- La pregunta clave (II) es: ¿con qué método numérico resuelvo la integral?
- La pregunta clave (III) es: ¿y si no hay método numérico? 

* Integración numérica

Recordemos la definición de integrales Reimann:

$$\int f(x) \text{d} x \approx \sum_{n=1}^N f(u_n) \Delta u_n =: \hat \pi_N^{\mathsf{R}} (f)\,.$$

#+BEGIN_NOTES
La aproximación utilizando una malla (cuadrícula) de $N$ puntos sería: 
$$\sum_{n=1}^N f(u_n) \Delta u_n.$$

El método es útil cuando las integrales se realizan cuando tenemos pocos
parámetros. Es decir, cuando el dominio de integración es $\mathcal{X} \subseteq \mathbb{R}^p$ con $p$ pequeña.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------------
#+end_src

#+begin_src R :exports none :results none

  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())

  ## Ejemplo de integracion numerica -----------------------

  grid.n          <- 11                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )


  norm.density <- tibble(x = seq(-5, 5, by = .01), 
         y = dnorm(x) ) 

#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature.jpeg :exports results :results output graphics file
  norm.cuadrature |>
    ggplot(aes(x=x + grid.size/2, y=y)) + 
    geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
    geom_bar(stat="identity", alpha = .3) + 
    geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
    sin_lineas + xlab('') + ylab("") + 
    annotate('text', label = expression(Delta~u[n]),
             x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
    annotate('text', label = expression(f(u[n]) ),
             x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
    annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
             x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
             angle = -90, alpha = .7, size = 12) + sin_ejes
#+end_src
#+caption: Integral por medio de discretización con $N = 11$.
#+RESULTS:
[[file:../images/quadrature.jpeg]]

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature-hi.jpeg :exports results :results output graphics file
  grid.n          <- 101                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )

  norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y)) + 
      geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
      sin_lineas + xlab('') + ylab("") + 
      annotate('text', label = expression(Delta~u[n]),
               x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
      annotate('text', label = expression(f(u[n]) ),
               x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
      annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
               x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
               angle = -90, alpha = .7, size = 12) + sin_ejes
#+end_src
#+caption: Integral por medio de una malla fina, $N = 101$. 
#+RESULTS:
[[file:../images/quadrature-hi.jpeg]]

** Análisis de error 

El concepto de ~integrabilidad de Darboux~ nos puede ayudar a acotar el error
cometido por nuestra estrategia de integración. Por ejemplo, para una partición $\rho_N$ (malla)
del intervalo tenemos que
\begin{align}
L_{f, \rho_N} \leq \hat \pi_N^{\mathsf{R}}(f) \leq U_{f, \rho_N}\,.
\end{align}

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature-darboux.jpeg :exports results :results output graphics file
    grid.n          <- 11                 # Número de celdas 
    grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
    norm.cuadrature <- tibble(x = seq(-5, 0, by = grid.size),
                              y.lo = dnorm(x - grid.size/2), y.hi = dnorm(x + grid.size/2))
    norm.density.half <- tibble(x = seq(-5, 0, by = .01), y = dnorm(x - grid.size/2) ) 

    g1 <- norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y.lo)) + 
      geom_area(data = norm.density.half, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.005), fill = 'black', stat="identity") + 
      sin_lineas + xlab('') + ylab("") + sin_ejes + xlim(-5,0)

    g2 <- norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y.hi)) + 
      geom_area(data = norm.density.half, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.005), fill = 'black', stat="identity") + 
      sin_lineas + xlab('') + ylab("") + sin_ejes + xlim(-5, 0)

    g1 + g2
#+end_src
#+caption: Integrales y cotas de Darboux. 
#+RESULTS:
[[file:../images/quadrature-darboux.jpeg]]

#+REVEAL: split
Lo que recordarán de sus cursos de cálculo es que
\begin{align}
\lim_{N \rightarrow \infty} |U_{f, \rho_N} - L_{f, \rho_N}| = 0\,,
\end{align}
y que además se satisface
\begin{align}
\int f(x) d x=\lim _{N \rightarrow \infty} U_{f, \rho_{N}}=\lim _{N \rightarrow \infty} L_{f, \rho_{N}}\,.
\end{align}

** Más de un parámetro

#+BEGIN_NOTES
Consideramos ahora un espacio con $\theta \in \mathbb{R}^p$. Si conservamos $N$
puntos por cada dimensión, ¿cuántos puntos en la malla necesitaríamos?  Lo que
tenemos son recursos computacionales limitados y hay que buscar hacer el mejor
uso de ellos. En el ejemplo, hay zonas donde no habrá contribución en la
integral.
#+END_NOTES


#+HEADER: :width 1500 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/eruption-quadrature.jpeg :exports results :results output graphics file
      canvas <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
       xlim(0.5, 6) +
       ylim(40, 110)

      grid.size <- 10 - 1

      mesh <- expand.grid(x = seq(0.5, 6, by = (6-.5)/grid.size),
                          y = seq(40, 110, by = (110-40)/grid.size))

    g1 <- canvas +
        geom_density_2d_filled(aes(alpha = ..level..), bins = 8) +
        scale_fill_manual(values = rev(color.itam)) + 
        sin_lineas + theme(legend.position = "none") +
        geom_point(data = mesh, aes(x = x, y = y)) + 
        annotate("rect", xmin = .5 + 5 * (6-.5)/grid.size, 
                  xmax = .5 + 6 * (6-.5)/grid.size, 
                  ymin = 40 + 3 * (110-40)/grid.size, 
                  ymax = 40 + 4 * (110-40)/grid.size,
                  linestyle = 'dashed', 
                 fill = 'salmon', alpha = .4) + ylab("") + xlab("") + 
        annotate('text', x = .5 + 5.5 * (6-.5)/grid.size, 
                         y = 40 + 3.5 * (110-40)/grid.size, 
                 label = expression(u[n]), color = 'red', size = 15) +
          theme(axis.ticks = element_blank(), 
              axis.text = element_blank())


    g2 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/grid.size, (110-40)/grid.size)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

    g3 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/25, (110-40)/25)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

  g1 + g2 + g3
#+end_src
#+caption: Integral multivariada por método de malla. 
#+RESULTS:
[[file:../images/eruption-quadrature.jpeg]]

** Reglas de cuadratura

Por el momento hemos escogido aproximar las integrales por medio de una aproximación con una ~malla uniforme~.
Sin embargo, se pueden utilizar aproximaciones 

$$\int f(x) \text{d} x \approx \sum_{n=1}^N f(\xi_n)\, \omega_n\,.$$

Estas aproximaciones usualmente se realizan para integrales en intervalos cerrados $[a,b]$. La regla de cuadratura determina los pesos $\omega_n$ y los centros $\xi_n$ pues se escogen de acuerdo a ~ciertos criterios de convergencia~.

#+BEGIN_NOTES
Por ejemplo, se consideran polinomios que aproximen con cierto grado de precisión el integrando. Los pesos y los centros se escogen de acuerdo a la familia de polinomios. Pues para cada familia se tienen identificadas las mallas que optimizan la aproximación. Ver sección 3.1 de citet:Reich2015. 
#+END_NOTES

* Integración Monte Carlo

\begin{gather*}
\pi(f) = \mathbb{E}_\pi[f] = \int f(x) \pi(x) \text{d}x\,,\\
\pi_N^{\textsf{MC}}(f) = \frac1N \sum_{n = 1}^N f( x^{(n)}), \qquad \text{ donde }  x^{(n)} \overset{\mathsf{iid}}{\sim} \pi, \qquad \text{ con } n = 1, \ldots, N \,, \\
 \pi(f) \approx \pi_N^{\textsf{MC}}(f)\,.
\end{gather*} 

** Ejemplo: Dardos

Consideremos el experimento de lanzar dardos uniformemente en un cuadrado de
tamaño 2, el cual contiene un circulo de radio 1.

#+HEADER: :width 1100 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-montecarlo.jpeg :exports results :results output graphics file
  ## Integración Monte Carlo ----------------------------------- 
  genera_dardos <- function(n = 100){
      tibble(x1 = runif(n, min = -1, max = 1), 
             x2 = runif(n, min = -1, max = 1)) %>% 
        mutate(resultado = ifelse(x1**2 + x2**2 <= 1., 1., 0.))
    }

    dardos <- tibble(n = seq(2,5)) %>% 
      mutate(datos = map(10**n, genera_dardos)) %>% 
      unnest() 

    dardos %>% 
      ggplot(aes(x = x1, y = x2)) + 
        geom_point(aes(color = factor(resultado))) + 
        facet_wrap(~n, nrow = 1) +  
      sin_lineas + sin_ejes + sin_leyenda + coord_equal()
#+end_src
#+caption: Integración Monte Carlo para aproximar $\pi$. 
#+RESULTS:
[[file:../images/dardos-montecarlo.jpeg]]

#+REVEAL: split
Si escogemos $N$ suficientemente grande entonces nuestro promedio converge a la
integral. En [[fig-mc-rolling]] se muestra para cada $n$ en el eje horizontal cómo
cambia nuestra estimación $\hat \pi_n^{\mathsf{MC}}(f)$ .

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-consistencia.jpeg :exports results :results output graphics file
  set.seed(1087)
  genera_dardos(n = 2**16) %>% 
    mutate(n = seq(1, 2**16), 
           approx = cummean(resultado) * 4) %>% 
    ggplot(aes(x = n, y = approx)) + 
      geom_line() + 
      geom_hline(yintercept = pi, linetype = 'dashed') + 
      scale_x_continuous(trans='log10', 
                         labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas

#+end_src
#+caption: Estimación $\pi_N^{\textsf{MC}}(f)$ con $N \rightarrow \infty$.
#+name: fig-mc-rolling
#+RESULTS:
[[file:../images/dardos-consistencia.jpeg]]


#+REVEAL: split
También podemos en replicar el experimento unas $M$ veces y observar cómo
cambiaría nuestra estimación con distintas semillas. Por ejemplo, podemos
replicar el experimento 10 veces. En ~R~ y ~python~ lo usual es utilizar ~arreglos
multidimensionales~ para poder guardar muestras bajo distintas replicas.

#+begin_src R :exports both :results org
  set.seed(108)
  nsamples <- 10**4; nexp <- 50
  U <- runif(nexp * 2 * nsamples)
  U <- array(U, dim = c(nexp, 2, nsamples))
  apply(U[1:5,,], 1, str)
#+end_src

#+RESULTS:
#+begin_src org
 num [1:2, 1:10000] 0.4551 0.7159 0.164 0.0627 0.5291 ...
 num [1:2, 1:10000] 0.404 0.2313 0.9282 0.0426 0.0883 ...
 num [1:2, 1:10000] 0.351 0.739 0.449 0.658 0.369 ...
 num [1:2, 1:10000] 0.664 0.984 0.627 0.762 0.185 ...
 num [1:2, 1:10000] 0.4635 0.6107 0.0115 0.7251 0.0117 ...
NULL
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  resultados <- apply(U, 1, function(x){
    dardos <- apply(x**2, 2, sum)
    exitos <- ifelse(dardos <= 1, 1, 0)
    prop   <- cummean(exitos)
    4 * prop
  })
#+end_src

#+REVEAL: split
Lo cual nos permite realizar distintos escenarios posibles. 
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-trayectorias.jpeg :exports results :results output graphics file
  resultados |>
    as_data_frame() |>
    mutate(n = 1:nsamples) |>
    pivot_longer(cols = 1:10) |>
    ggplot(aes(n, value)) +
    geom_line(aes(group = name, color = name)) +
    geom_hline(yintercept = pi, linetype = 'dashed') + 
    scale_x_continuous(trans='log10', 
                       labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas + sin_leyenda +
    ylim(0, 7)
#+end_src
#+caption: Réplica de las trayectorias de diversas realizaciones de la aproximación de la integral.
#+RESULTS:
[[file:../images/dardos-trayectorias.jpeg]]

#+REVEAL: split
Bajo ciertas consideraciones teóricas podemos esperar un buen comportamiento de
nuestro estimador de la integral. E incluso podríamos (si el número de
simulaciones lo permite) aproximar dicho comportamiento utilizando
distribuciones asintóticas, ($\mathsf{TLC}$).

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-normalidad.jpeg :exports results :results output graphics file
  resultados |>
    as_data_frame() |>
    mutate(n = 1:nsamples) |>
    pivot_longer(cols = 1:nexp) |>
    group_by(n) |>
    summarise(promedio = mean(value),
              desv.est = sd(value),
              y.lo = promedio - 2 * desv.est,
              y.hi = promedio + 2 * desv.est) |>
    ggplot(aes(n , promedio)) +
    geom_ribbon(aes(ymin = y.lo, ymax = y.hi), fill = "gray", alpha = .3) +
    geom_hline(yintercept = pi, linetype = 'dashed') + 
    geom_line() +
    scale_x_continuous(trans='log10', 
                       labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas + sin_leyenda +
  ylim(0, 7)
#+end_src
#+caption: Comportamiento promedio e intervalos de confianza. 
#+RESULTS:
[[file:../images/dardos-normalidad.jpeg]]

** Propiedades

A continuación enunciaremos algunas propiedades clave del método Monte
Carlo. Poco a poco las iremos explicando y en particular discutiremos mas a
fondo algunas de ellas. 

*** ~Teorema~ [Error Monte Carlo]
Sea $f : \mathbb{R}^p \rightarrow \mathbb{R}$ cualquier función bien
comportada$^\dagger$.  Entonces, el estimador Monte Carlo es *insesgado*. Es
decir, se satisface

\begin{align}
\mathbb{E}\left[\hat  \pi_N^{\textsf{MC}}(f) - \pi(f)\right] = 0,
\end{align}
para cualquier $N$. Usualmente estudiamos el error en un escenario pesimista
donde medimos el *error cuadrático medio* en el peor escenario

\begin{align*}
\sup_{f \in \mathcal{F}} \, \,  \mathbb{E}\left[ \left(\hat \pi_N^{\textsf{MC}}(f) - \pi(f) \right)^2 \right] \leq \frac1N.
\end{align*}

#+BEGIN_NOTES
Esta desigualdad nos muestra una de las propiedades que usualmente se celebran
de los métodos Monte Carlo. La integral y nuestra aproximación de ella por medio
de simulaciones tiene un error acotado proporcionalmente por el número de
simulaciones.
#+END_NOTES

#+REVEAL: split
En particular, la varianza del estimador (*error estándar*) satisface la igualdad

$$ \textsf{ee}^2\left(\hat \pi_N^{\textsf{MC}}(f)\right) = \frac{\mathbb{V}_\pi( f )}{N}.$$

#+BEGIN_NOTES
Esta igualdad, aunque consistente con nuestra desigualdad anterior, nos dice
algo mas. El error de nuestra aproximación *depende* de la varianza de $f$ bajo la
distribución $\pi$.
#+END_NOTES

*** ~Teorema~ [TLC para estimadores Monte Carlo]
Sea $f$ una función *bien comportada* $^{\dagger\dagger}$, entonces bajo una $N$
suficientemente grande tenemos
\begin{align}
\sqrt{N} \left(\hat \pi_N^{\textsf{MC}} (f) - \pi(f) \right) \sim \mathsf{N}\left(0, \mathbb{V}_\pi(f)\right)\,.
\end{align}

*** ~Nota~:
El estimador Monte Carlo del que hablamos, $\hat \pi_{N}^{\mathsf{MC}}(f)$, es una estimación con una ~muestra finita de simulaciones~. En ese sentido podemos pensar que tenemos un /mapeo/ de muestras a estimador
\begin{align}
(x^{(1)}, \ldots, x^{(N)}) \mapsto  \hat \pi_N^{\mathsf{MC}}(f)\,,
\end{align}
con $x^{(i)} \overset{\mathsf{iid}}{\sim} \pi$ . 

#+REVEAL: split
De lo cual es natural pensar: ¿y si hubiéramos observado otro conjunto de
simulaciones? Nuestro proceso de estimación es el mismo pero la muestra puede
cambiar.

#+REVEAL: split
En este sentido nos preguntamos por el ~comportamiento promedio~ bajo distintas
muestras observadas
\begin{align}
\mathbb{E}[\hat \pi_N^{\mathsf{MC}}(f)] = \mathbb{E}_{x_{1}, \ldots, x_{N}}[\hat \pi_N^{\mathsf{MC}}(f)]\,.
\end{align}
De la misma manera nos podemos preguntar sobre la ~dispersión alrededor de dicho
promedio~ (varianza)
\begin{align}
\mathbb{V}[\hat \pi_N^{\mathsf{MC}}(f)] = \mathbb{V}_{x_{1}, \ldots, x_{N}}[\hat \pi_N^{\mathsf{MC}}(f)]\,.
\end{align}

#+REVEAL: split
Al ser un ejercicio de ~estimación~ la desviación estándar del estimador recibe el
nombre de ~error estándar~. Lo cual denotamos por
\begin{align}
\mathsf{ee}[\hat \pi_N^{\mathsf{MC}}(f)] = \left( \mathbb{V}[\hat \pi_N^{\mathsf{MC}}(f)]  \right)^{1/2}= \left(  \frac{\mathbb{V}_\pi( f )}{N} \right)^{1/2}\,.
\end{align}

*** ~Nota~:
Para algunos estimadores la fórmula del error estándar se puede obtener de
manera analítica (curso de ~Inferencia Matemática~). Para otro tipo, tenemos que
utilizar propiedades asintóticas (p.e. cota de Cramer-Rao).

#+REVEAL: split
Hay casos en los que no existe una fórmula asintótica o resultado analítico, pero
podemos usar simulación [ ~8)~ ] para cuantificar dicha dispersión (lo veremos en
otra sección del curso).

*** ~Nota~:
Hay situaciones en las que la distribución normal asintótica no tiene
sentido. Para este tipo de situaciones también veremos cómo podemos utilizar
simulación para cuantificar dicha dispersión.

#+DOWNLOADED: screenshot @ 2022-08-29 19:52:47
#+attr_html: :width 700 :align center
#+caption: Comportamiento promedio e intervalos de confianza con aproximación asintótica.
[[file:../images/dardos-normalidad.jpeg]]


** Estimación de una proporción

El lanzamiento de dardos que vimos es un ejemplo de una situación muy usual en
estimación de integrales. Queremos estimar la tasa de éxito a partir de ver el
éxito o fracaso de experimentos Bernoulli.

#+REVEAL: split
Si denotamos por $\theta$ la tasa de éxito. Entonces nuestro experimento (lanzar dados dentro del círculo) determina que $S_n \sim \mathsf{Binomial}(N, \theta)$ y que $\bar X_n$ es un *estimador* de $\theta$. Por lo tanto,
\begin{align}
\hat \theta_n := \bar X_n \approx \theta
\end{align}

*** ~Tarea~:
:PROPERTIES:
:reveal_background: #00468b
:END:

¿Cuál es la fórmula del error estándar para este estimador?

*** ~Pregunta~:
¿Cuántas muestras necesitamos para tener una aproximación /buena/?

* Desigualdades de concentración 

En muchas situaciones nos interesa establecer cuántas simulaciones necesitamos
para poder aproximar nuestras integrales hasta cierto orden. Por ejemplo, la
tabla en ref:tab-darts muestra la aproximación mediante una muestra conforme aumenta
$N$.

#+begin_src R :exports results :results org
  tibble(N = 1:nsamples, estimado = resultados[,1]/4) |>
    mutate( dif.abs = abs(estimado - pi)/4) |>
    filter(N %% 10 == 0 & log10(N) %in% c(1, 2, 3, 4)) |>
    as.data.frame()
#+end_src
#+name: tab-darts
#+caption: Aproximación de la proporción de dardos dentro de la diana.
#+RESULTS:
#+begin_src org
      N estimado dif.abs
1    10   1.0000  0.5354
2   100   0.8000  0.5854
3  1000   0.7920  0.5874
4 10000   0.7876  0.5885
#+end_src

** Desigualdad de Chebyshev

Lo que queremos es encontrar una $N$ tal que con una ~alta probabilidad~ nuestro
~estimador sea cercano al parámetro~ que está ajustando. Esto lo escribimos como
\begin{align}
\mathsf{Prob} \left( |\hat \theta_N - \theta| < \epsilon \right) \geq \alpha
\end{align}

*** ~Teorema~ [Desigualdad de Chebyshev]:
Sea $X$ una variable aleatoria con media y varianza finita denotadas por $\mu$ y
$\sigma^2$ respectivamente. Entonces para cualquier constante positiva $k \in
\mathbb{R}$, tenemos que
\begin{align}
\mathsf{Prob}\left( |X - \mu| \geq k \, \sigma\right) \leq \frac{1}{k^2}\,.
\end{align}

#+REVEAL: split
Lo cual podemos utilizar para encontrar una cota inferior para $N$.

*** ~Ejercicio~:
:PROPERTIES:
:reveal_background: #00468b
:END:

Calcula la desigualdad y obten el número de simulaciones necesarios para
encontrar un estimador con nivel de precisión $\epsilon$ con una probabilidad
$\alpha$.

*** ~Solución~:                                                     :latex:


** Desigualdad de Hoeffding




bibliographystyle:abbrvnat
bibliography:references.bib


* Fuera                                                            :noexport:
** Ejemplo: Proporciones                                          :noexport:

Consideramos la estimación de una proporción $\theta$, tenemos como inicial
$p(\theta) \propto \theta$, que es una $\mathsf{Beta}(2,1)$. Si observamos 3
éxitos en 4 pruebas, entonces sabemos que la posterior es $p(\theta|x)\propto
\theta^4(1-\theta)$, que es una $\mathsf{Beta}(5, 2)$. Si queremos calcular la
media y el segundo momento posterior para $\theta$, en teoría necesitamos
calcular

\begin{align}
\mu_1 = \int_0^1 \theta \,\, p(\theta|X = 3)\, \text{d}\theta,\qquad  \mu_2=\int_0^1 \theta^2 \,\, p(\theta|X = 3)\, \text{d}\theta.
\end{align}

#+REVEAL: split
#+begin_src R :exports none :results none
  ### Ejemplo proporciones ------------------ 
#+end_src

Utilizando el ~método Monte Carlo~: 
#+begin_src R
theta <- rbeta(10000, 5, 2)
media_post <- mean(theta)
momento_2_post <- mean(theta^2)
c(mu_1 = media_post, mu_2 = momento_2_post)
#+end_src

#+RESULTS:
#+begin_src org
mu_1 mu_2 
0.71 0.54
#+end_src

#+REVEAL: split
Incluso, podemos calcular cosas mas /exóticas/ como
\begin{align}
P(e^{\theta}> 2|x)\,.
\end{align}

#+begin_src R
mean(exp(theta) > 2)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.61
#+end_src

** Ejemplo: Sabores de helados                                    :noexport:

Supongamos que probamos el nivel de gusto para 4 sabores distintos de una
paleta. Usamos 4 muestras de aproximadamente 50 personas diferentes para cada
sabor, y cada uno evalúa si le gustó mucho o no. Obtenemos los siguientes
resultados:
#+begin_src R :exports none :results none
  ### Ejemplo helados ------------------------- 
#+end_src

#+begin_src R :exports results
  datos <- tibble(
    sabor = c("fresa", "limon", "mango", "guanabana"),
    n = c(50, 45, 51, 50), gusto = c(36, 35, 42, 29)) %>% 
    mutate(prop_gust = gusto / n)

  datos |>
  as.data.frame()
#+end_src

#+caption: Resultados de las encuestas.
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust
1     fresa 50    36      0.72
2     limón 45    35      0.78
3     mango 51    42      0.82
4 guanábana 50    29      0.58
#+end_src

#+REVEAL: split
Usaremos como inicial $\mathsf{Beta}(2, 1)$ (pues hemos obervado cierto sesgo de
cortesía en la calificación de sabores, y no es tan probable tener valores muy
bajos) para todos los sabores, es decir $p(\theta_i)$ es la funcion de densidad
de una $\mathsf{Beta}(2, 1)$. La inicial conjunta la definimos entonces, usando
~independencia inicial~, como

$$p(\theta_1,\theta_2, \theta_3,\theta_4) = p(\theta_1)p(\theta_2)p(\theta_3)p(\theta_4)\,.$$

#+REVEAL: split
Pues inicialmente establecemos que ningún parámetro da información sobre otro:
saber que mango es muy gustado no nos dice nada acerca del gusto por fresa. Bajo
este supuesto, y el supuesto adicional de que las muestras de cada sabor son
independientes, podemos mostrar que las ~posteriores son independientes~:

$$p(\theta_1,\theta_2,\theta_3, \theta_4|k_1,k_2,k_3,k_4) = p(\theta_4|k_1)p(\theta_4|k_2)p(\theta_4|k_3)p(\theta_4|k_4)$$

#+REVEAL: split
#+begin_src R :exports results
  datos <- datos |>
    mutate(a_post = gusto + 2,
           b_post = n - gusto + 1,
           media_post = a_post/(a_post + b_post))
  datos |>
    as.data.frame()
#+end_src

#+caption: Resultado de inferencia Bayesiana. 
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust a_post b_post media_post
1     fresa 50    36      0.72     38     15       0.72
2     limón 45    35      0.78     37     11       0.77
3     mango 51    42      0.82     44     10       0.81
4 guanábana 50    29      0.58     31     22       0.58
#+end_src

#+REVEAL: split
Podemos hacer preguntas interesantes como: ¿cuál es la probabilidad de que mango
sea el sabor preferido?  Para contestar esta pregunta podemos utilizar
simulación y responder por medio de un procedimiento Monte Carlo.

#+begin_src R :exports none :results none
  modelo_beta <- function(params, n = 5000){
    rbeta(n, params$alpha, params$beta)
  }
#+end_src

#+begin_src R :exports code :results none
  ## Generamos muestras de la posterior
  paletas <- datos |>
    mutate(alpha = a_post, beta = b_post) |>
    nest(params.posterior = c(alpha, beta)) |>
    mutate(muestras.posterior = map(params.posterior, modelo_beta)) |>
    select(sabor, muestras.posterior)
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/paletas-hist.jpeg :exports results :results output graphics file
  paletas |>
    unnest(muestras.posterior) |>
    ggplot(aes(muestras.posterior)) +
    geom_histogram(aes(fill = sabor), position = "identity" ) +
    sin_lineas
#+end_src
#+caption: Histogramas de la distribución predictiva marginal para cada $\theta_j$. 
#+RESULTS:
[[file:../images/paletas-hist.jpeg]]

#+REVEAL: split
#+begin_src R
  ## Utilizamos el metodo Monte Carlo para aproximar la integral. 
  paletas |>
    unnest(muestras.posterior) |>
    mutate(id = rep(seq(1, 5000), 4)) |> group_by(id) |>
    summarise(favorito = sabor[which.max(muestras.posterior)]) |>
    group_by(favorito) |> tally() |>
    mutate(prop = n/sum(n)) |>
    as.data.frame()
#+end_src
#+caption: Aproximación Monte Carlo.
#+RESULTS:
#+begin_src org
   favorito    n   prop
1     fresa  308 0.0616
2 guanábana    1 0.0002
3     limón 1319 0.2638
4     mango 3372 0.6744
#+end_src

#+BEGIN_NOTES
Escencialmente estamos preguntándonos sobre calcular la integral:
\begin{align}
\mathbb{P}(\text{mango sea preferido}) = \int_\Theta f(\theta_1, \ldots, \theta_4) \, p(\theta_1, \ldots, \theta_4 | X_1, \ldots, X_n) d\theta\,,
\end{align}
donde $f(\theta_1, \ldots, \theta_4) = \mathbb{I}_{[\theta_4 \geq \theta_j, j \neq 4]}(\theta_1, \ldots, \theta_4)$. 
#+END_NOTES

** Tarea: Sabores de helados                                      :noexport:

- ¿Cuál es la probabilidad a priori de que cada sabor sea el preferido?
- Con los datos de arriba, calcula la probabilidad de que la gente prefiera el sabor de mango sobre limón.

* Extensiones: Muestreo por importancia                            :noexport:

Incluso cuando tenemos una integral *complicada* podemos ~relajar~ el problema de integración. De tal forma que podemos ~sustituir~
$$\int f(x) \pi(x) \text{d} x = \int f(x) \frac{\pi(x)}{\rho(x)}\,\rho(x) \text{d} x = \int f(x) \, w(x) \, \rho(x) \, \text{d}x\,,$$
donde $\rho$ es una densidad de una variable aleatoria ~adecuada~.

#+REVEAL: split
Esto nos permite utilizar lo que sabemos de las propiedades del método Monte Carlo para resolver la integral
\begin{align*}
\pi(f) =  \int f(x) \pi(x) \text{d} x = \int f(x) w(x) \, \rho(x) \, \text{d}x =: \rho(fw)\,,
\end{align*}
por medio de una aproximación
\begin{align}
\pi(f) \approx \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad x^{(n)} \overset{\mathsf{iid}}{\sim} \rho\,.
\end{align}
#+REVEAL: split
Al estimador le llamamos el estimador por importancia y lo denotamos por
\begin{align}
\pi_N^{\mathsf{IS}}(f) = \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad \bar w^{(n)} = \frac{w(x^{(n)})}{\sum_{m= 1}^{N}w(x^{(m)})}\,.
\end{align}

** Propiedades: muestreo por importancia

Lamentablemente, utilizar muestreo por importancia ~impacta la calidad de la
estimación~ (medida, por ejemplo, en términos del *peor error cuadrático medio
cometido*). El impacto es un factor que incorpora la /diferencia/ entre la distribución
~objetivo~ --para integrales de la forma $\int f(x) \text{d}x$, implica la
distribución uniforme-- y la distribución ~sustituto~. Puedes leer más de esto
(aunque a un nivel mas técnico) en la sección 5 de las notas de
citet:Sanz-Alonso2019.

** Ejemplo

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-importancia-mezcla.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
      function(x){
        weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
          weights$w2 * dnorm(x, mean = 1.5, sd = .7)
      }
    }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  muestras_mezcla <- function(id){
    n <- 100
    tibble(u = runif(n)) |>
      mutate(muestras = ifelse(u <= .6,
                               rnorm(1, -1.5, sd = .5),
                               rnorm(1,  1.5, sd = .7))) |>
      pull(muestras)
  }

  muestras.mezcla <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_mezcla)) |>
    unnest(muestras) |>
    group_by(id) |>
    summarise(estimate = mean(muestras))

  g0 <- muestras.mezcla |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.mezcla$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Objetivo")

  muestras_uniforme <- function(id){
    n <- 100
    runif(n, -5, 5)
  }

  muestras.uniforme <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_uniforme)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dunif(muestras, -5, 5),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g1 <- muestras.uniforme |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.uniforme$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Uniforme(-5,5)")

  muestras_importancia <- function(id){
    n <- 100
    rnorm(n, 0, sd = 1)
  }  

  muestras.normal  <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_importancia)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dnorm(muestras, 0, sd = 1),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g2  <- muestras.normal |> ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.normal$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Normal(0, 2)")

  g0 + g1 + g2

#+end_src
#+caption: Muestreo por importancia utilizando distintas distribuciones instrumentales. Distribución /bootstrap/ de muestreo con $B = 10,000$ y $n = 100$. 
#+RESULTS:
[[file:../images/muestreo-importancia-mezcla.jpeg]]

#+BEGIN_NOTES
El análisis  del error en la sección anterior habla en del error cuadrático medio en el peor escenario posible bajo una familia de funciones de prueba (resumen). El ejemplo anterior muestra el error Monte Carlo cometido con respecto a una función resumen $f(\theta) = \theta$ con la cual, vemos, se reduce la varianza. Esto no contradice lo anterior pues para esta función resumen nuestra distribución instrumental satisface el criterio de reducción de varianza. En general, lo complicado es encontrar dicha distribución que podamos usar en la estimación Monte Carlo. 
#+END_NOTES


bibliographystyle:abbrvnat
bibliography:./references.bib
